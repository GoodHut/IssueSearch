 URL : "https://github.com/palantir/atlasdb/issues/2194" TITLE : disable okhttp retries BODY : we've seen issues in the field recently with http/2 connections apparently getting stuck in a retry loop. i've also been able to unreliably reproduce this by simulating heavy load and connection errors. however, today i came across this issue after doing a rolling bounce. there was no load on the stack; only background sweep. a few minutes later two of the nodes had high cpu usage. timelock-1 was showing 95% usr cpu 90 threads stuck in latestsequencepreparedoraccepted 2 threads in getlearnedvaluessince timelock-3 shows 45% usr cpu 4 threads in getlearnedvaluessince all of the above threads had similar stack traces: - java.lang.throwable.fillinstacktrace int @bci=0 compiled frame; information may be imprecise - java.lang.throwable.fillinstacktrace @bci=16, line=783 compiled frame - java.lang.throwable.<init> java.lang.string @bci=24, line=265 compiled frame - java.lang.exception.<init> java.lang.string @bci=2, line=66 compiled frame - java.io.ioexception.<init> java.lang.string @bci=2, line=58 compiled frame - okhttp3.internal.framed.framedconnection.newstream int, java.util.list, boolean, boolean @bci=49, line=238 compiled frame - okhttp3.internal.framed.framedconnection.newstream java.util.list, boolean, boolean @bci=5, line=224 compiled frame - okhttp3.internal.http.http2xstream.writerequestheaders okhttp3.request @bci=53, line=134 compiled frame - okhttp3.internal.http.callserverinterceptor.intercept okhttp3.interceptor$chain @bci=32, line=42 compiled frame - okhttp3.internal.http.realinterceptorchain.proceed okhttp3.request, okhttp3.internal.connection.streamallocation, okhttp3.internal.http.httpstream, okhttp3.connection @bci=205, line=92 compiled frame i think it should be fine to disable retrying at the okhttp level, as we already have retry logic at the feign level.