 URL : "https://github.com/Infinidat/infi.clickhouse_orm/issues/26" TITLE : very long inserts BODY : bulk inserts work very slow via orm. i insert 1k of rows into table. here is the time measure of code: t2 = time.time statsd.timing statsd_key_prefix + '.import_clickhouse_model_objects.convert', t2 - t1 for model_group in convert_items.values : settings.clickhouse_db.insert model_group, batch_size=batch_size t3 = time.time i know, that convert_items dict contains only 1 item. so, in fact, only insert is measured. batch_size is 1000 here. there are 1000 elements in the list. i've also tried making batch_size=10000, but the query failed with memoryerror on backend size... clickhouse server is not loaded at all 8 cores, 16gb memory . htop doesn't show any activity practically. here is the graph, where you can see inserts are executed 60-100 seconds... very very slow. ! screenshot https://cloud.githubusercontent.com/assets/2315339/24703337/43e4b60c-1a1c-11e7-8526-6fcfc872a6b3.png another query in my code makes insert from select via settings.clickhouse_db.raw query . it inserts 200 rows and executes for 1-2 seconds only