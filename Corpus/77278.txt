 URL : "https://github.com/vlfeat/matconvnet/issues/1040" TITLE : multi-pass loss function BODY : i am trying to implement the pi-ensemble model from the paper: laine, s. and t. aila. temporal ensembling for semi-supervised learning, arxiv:1610.02242, march 2017. the pi-model requires you to send the same mini-batch of images through your network for two forward passes. on each forward pass different data augmentations and dropout are used. the resulting softmax outputs from the two forward passes are then differenced to form an unsupervised loss that is combined with standard cross-entropy loss and back propagated through the network to update the network weights. how would i go about creating such a loss layer that spans two different forward passes? i could replicate the whole network, making sure that i share the weights, and then inject my two different versions of the same mini-batch of images through each side of the network - then i can see how to create the loss layer that i would need. but this seems like an ugly implementation. is there a better way?