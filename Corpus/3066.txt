 URL : "https://github.com/kohesive/elasticsearch-data-import-handler/issues/39" TITLE : huge data set BODY : if i import a huge data set from sql, will it need to first create a temporary spart table before ingesting into es? or does it chunk things up so it does not require huge amount of ram and/or disk while importing? also, is there some way to support full vs incremental import, i.e. different sql statement to only select what has changed since last ingest?