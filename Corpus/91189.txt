 URL : "https://github.com/deepchem/deepchem/issues/681" TITLE : tag device in tensorgraph BODY : we are starting to deploy deepchem in server environments with multiple gpu's per node. we wouldn't have to do this if our server environment implement proper cgroups or containerization -- but alas i think this will be standard across the industry. i want to add a constructor arg for tensorgraph for the device so it won't try to take up a gpu that is supposed to be doing something else say a dft calculation . this will not help with multi-gpu training for models, but is this actually a problem we realistically expect to have in the next year?