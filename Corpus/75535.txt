 URL : "https://github.com/fchollet/keras/issues/7551" TITLE : how does tokenizer work? BODY : hi, i am currently working with the tokenizer class and i have a question about the relevance of num_words. reading the documentation suggests that when .fit_on_texts is run the tokenizer will only take the most common num_words amounts. i currently have a dataset consisting of 10358 uniques words. when i run tokenizer specifying num_words = 1000 i then call the word index which has a length of 10358. does the tokenizer generate an index of the top 1000 then add any others on after this when running fit_on_texts? thanks