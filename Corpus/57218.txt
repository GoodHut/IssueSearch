 URL : "https://github.com/reidmcy/MACS30200proj/issues/4" TITLE : poster peer evaluation BODY : poster presentation is great. awesome neutral color scheme, symmetrically embedded sections and titles, and good figure captions. small note, there is a typo: since the postives are greatly outnumber ed by the negatives . the only negative things i can say for the layout is that your rnn layer output plot the y labels for words are overlapping, making it difficult to read some of the words, the statistical software labels are cut off in publication vs number of from each class , and it's a little strange having the per year result across all papers table on the opposite side of your data description. additionally, for your question, i'm a little confused one a certain aspect. you say that there is no hand coding, but in that case, how were the articles identified as having introduced new software or not? looking at your data section, my best guess is that articles from the three journals that primarily introduce new software were all considered as introducing new software versus the eight that didn't being labeled as _not_. this seems to make sense, especially when you indicate that errors resulted from the data and not the model, but it would be nice for this to be made more explicit.