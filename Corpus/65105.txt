 URL : "https://github.com/dfridovi/rl/issues/6" TITLE : implement gp q-learning BODY : the basic idea is to represent the joint state-action value function as a gaussian process. the optimal policy can be approximated with a few steps of gradient descent on the action subspace, holding state fixed. a few ideas here: can start with a random sampling of points across state/action space, with randomly initialized means learn the training means of the gp by a few steps of gradient descent against a batch might be some interesting convergence arguments to be made some extensions: 1. could improve accuracy by adding training points to the gp, for example if a radius search has fewer than n neighbors. 2. could update only some training means, e.g. those returned by a radius search around a single query point i.e. batch size 1 only . 3. exploration could be done by, e.g., upper confidence bounding in the sub optimal policy computation.