 URL : "https://github.com/fchollet/keras/issues/7803" TITLE : multi head attention layer BODY : i think is a good idea start to think how to implement this sort of layer in keras. i know that is a really fresh algorithm, but i believe that's a new cutting edge tech in deep learning for the next years. paper: attention is all you need https://arxiv.org/abs/1706.03762 blog showing some results: google research blog https://research.googleblog.com/2017/08/transformer-novel-neural-network.html?m=1 tensor2tensor library tensor2tensor https://github.com/tensorflow/tensor2tensor pytorch implementation pytorch-t2t https://github.com/jadore801120/attention-is-all-you-need-pytorch