 URL : "https://github.com/deepmind/pysc2/issues/100" TITLE : troubleshooting a3c agent BODY : hello, as a part of my master thesis, i have been trying to reproduce the results from the deepmind paper. i have implemented the a3c algorithm and am currently testing it on the movetobeacon minigame. however, i am having troubles with learning a good policy. none of my runs have exceeded average reward of 2. however, i have pretty limiting hardware; i am not able to run more than 3600 simulation steps per minute including the loss function minimisation and having multiple simulations running at the same time, 1 step = 8 actual game steps . so it takes nearly 5 hours of training to get to 1 million simulation steps. however, i am not sure what i am doing wrong. it is possible that there is somewhere some mistake in my implementation it should be in accordance with the a3c original paper . it is possible that i have set wrong hyperparameters. it is possible that the learning is so slow that it seems that no learning happens i usually don't let it run much past 1 million steps . but i can't tell with certainty which one it is until i successfully train an agent. in the paper, you mention some of the hyper-parameter settings learning rate from uniform 1e-3,1e-5 , entropy loss coef 1e-3, 40 steps unroll of bptt, 64 threads . however, you don't mention some of the others, specifically, from what distribution have you sampled the value gradient coefficient and the t_max of a3c meaning the max number of steps for computing the n-step reward ? how important is the number of parallel agents training? to me, there seems to be little difference in training with 10 agents and with 32 agents. also, what would be your recommendation on determining where is the problem with my implementation? how fast should the correct algorithm be expected to find a reasonable policy on the movetobeacon minigame? in another issue, someone said that after couple tens of thousands, it should already be good. any insight greatly appreciated!