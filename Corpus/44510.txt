 URL : "https://github.com/datajoint/datajoint-python/issues/425" TITLE : external capabilities BODY : i have some questions regarding the _external_ field and function. i am mostly interested in its implementation in python. 1. for very large datasets it would be useful to have the option to perform partial loading / memory mapping of data when it is fetched - is there anything available/planned like this? 2. how complex can arrays become that are written to the external field - are there any limitations? i understand that the aim here is to increase compatibility between python matlab and arbitrary datatypes are not supported? 3. is there a way to use the _external_ functionality storage handling, md5 hash creation, ... for a file that hasn't been loaded and serialised using the datajoint interface for example an arbitrary binary file . as a concrete example we want to write out converted and original data to external storage, but organise in the same schema.