 URL : "https://github.com/ryanleary/pytorch-ctc/issues/13" TITLE : confusion about trie files BODY : i'm unclear as to what to expect from the generated trie files. i've run generate_lm_trie.py from deepspeech.pytorch https://github.com/seannaren/deepspeech.pytorch , as well as directly within a python shell, by importing pytorch_ctc. with the former, the process takes around one second tested with: 17gb/50k-vocab/5-gram, 2.5gb/50k-vocab/3-gram, and 6.2gb/100k-vocab/5-gram kenlm binaries , producing <3kb trie files with exactly 869 lines of mostly -1s and 0s on every third line up to line 74 , with no error messages. with the latter, the process takes some 20-30 minutes, producing ~10mb files, with 3.45m lines still of mostly -1s for the two aforementioned 50k-vocab binaries. what are the expected formats and sizes of the trie files, and is there some reference against which i might compare mine? secondly, using the parameters quoted in the table here https://github.com/seannaren/deepspeech.pytorch/issues/76 beam_width 100, lm_alpha 4.0, lm_beta1 0.0, lm_beta2 5.0 increase wer/cer on a test set i'm using from 26.86/10.35 with greedy or argmax decoders to 100.00/29.63. i have not gridsearched, but i haven't found any configurations that improve wer or cer. is there something obvious i'm missing? thanks!