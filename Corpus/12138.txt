 URL : "https://github.com/ematvey/tensorflow-seq2seq-tutorials/issues/18" TITLE : my own embedding BODY : i am new to tensorflow. so pardon me if some of these things are too basic. i just wanted to confirm what changes will i have to do in case i want to use my own encoding. as you told, i have the input vector of max_time, batch_size, embedding_size . here is my thought about this. i will remove these lines: encoder_inputs = tf.placeholder shape= none, none , dtype=tf.int32, name='encoder_inputs' decoder_targets = tf.placeholder shape= none, none , dtype=tf.int32, name='decoder_targets' decoder_inputs = tf.placeholder shape= none, none , dtype=tf.int32, name='decoder_inputs' instead of these lines: embeddings = tf.variable tf.random_uniform vocab_size, input_embedding_size , -1.0, 1.0 , dtype=tf.float32 encoder_inputs_embedded = tf.nn.embedding_lookup embeddings, encoder_inputs decoder_inputs_embedded = tf.nn.embedding_lookup embeddings, decoder_inputs i'll put these: encoder_inputs_embedded = tf.placeholder shape= time, none, 4 , dtype=tf.float32, name='encoder_embeddings' decoder_inputs_embedded = tf.placeholder shape= time, none, 4 , dtype=tf.float32, name='encoder_embeddings' but my doubt is in the projection layer. you are projecting it to vocabulary size although i have the embedding size. so, if i change everything to embedding_size, will the logic and working of the code still remain the same? thanks