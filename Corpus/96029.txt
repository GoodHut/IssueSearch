 URL : "https://github.com/fchollet/keras/issues/7290" TITLE : dropout in embedding layer BODY : in this paper https://arxiv.org/pdf/1512.05287.pdf , the authors state that applying dropout to the input of an embedding layer by selectively dropping certain ids is an effective method for preventing overfitting. for example, if the embedding is a word2vec embedding, this method of dropout might drop the word the from the entire input sequence. in this case, the input the dog and the cat would become -- dog and -- cat . the input would never become -- dog and the cat . this is useful to prevent the model from depending on certain words. although keras currently allows for applying dropout to the output vector of an embedding layer, as far as i can read from the documents, it does not allow for applying dropout selectively to certain ids. since embeddings are frequently used, and the above paper states that embeddings are prone to overfitting, this feature seems to be a feature that would be useful for a relatively wide range of users. the expected api would be something like from keras.layers import embedding embedding = embedding x, y, dropout=0.2 where the dropout rate signifies the rate of ids to drop. would this be a worthy feature to add? or is there a relatively obvious way to implement this functionality already?