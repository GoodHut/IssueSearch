 URL : "https://github.com/2017-fall-DL-training-program/ConvNetwork/issues/13" TITLE : learning rate warm up BODY : i quote one paragraph in kaiming he's paper, we further explore n = 18 that leads to a 110-layer resnet. in this case, we find that _the initial learning rate of 0.1 is slightly too large to start converging_ . so we _use 0.01 to warm up_ the training _until the training error is below 80%_ about 400 iterations , and then go back to 0.1 and continue training. the rest of the learning schedule is as done previously. _this 110-layer network converges well_ fig. 6, middle . in the experiments, i did not see the training error over 80% in the beginning epochs such that i did not use this warm-up scheme. however, i saw resnet-110 indeed converge slowly than resnet-56 in the beginning. until epoch 18, resnet-110 training error can beat resnet-56 then. my question is shall we apply warm-up scheme to make resnet-110 converge soon in earlier epochs ?