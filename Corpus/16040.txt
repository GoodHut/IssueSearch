 URL : "https://github.com/deepchem/deepchem/issues/639" TITLE : gru layer with rl BODY : i want to use a tensorgraph gru layer as part of a model for reinforcement learning. it's not clear how to make that work. i'm pretty sure it's going to need some modifications, but i'm not certain of the best way to do it. the gru layer doesn't expose its internal state in any way. every time you call session.run , it reinitializes the state to all zeros, runs that batch through it, and throws away the final state. but for rl, we call run repeatedly to select the action for the next step, and we somehow need to be able to preserve that state from one step to the next. separately from that, we also sometimes call run to evaluate the loss on a rollout, and in that case it is appropriate to reinitialize the state to zero for the start of the rollout. how can we modify the gru layer to make it work with this?