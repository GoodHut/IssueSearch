 URL : "https://github.com/linnarsson-lab/loompy/issues/22" TITLE : hdf5 performance tuning BODY : split out from 19 to avoid that topic from losing too much focus. so i have spent some time in the past trying to tune the hdf5 settings of loompy see here https://github.com/linnarsson-lab/loom-viewer/issues/90 and here https://github.com/jobleonard/loompy-chunk-cache-experiments/blob/master/loompy%20cache%20test.ipynb , looking for the best trade-off in terms of compression size and disk access speed. the very weird result so far is that regardless of the overrides i have tried for the chunk-cache, loading times remain the same, even if i set it to zero. i am on a maching with 64gb of ram, i should be able to blow up the chunk cache to crazy numbers, and given the 64x64 chunk size see 64x less disk access. but nothing happens. i'm thinking of opening a topic on the h5py google groups to help me figure out what is wrong here, because the numbers make no sense. i guess that at the moment everything uses the default 1mb chunk cache, despite overrides. this is _ridiculously_ low on modern machines, and far from adequate for larger data sets assuming 64x64 chunks and 32bit floats, you cannot even load a single row without evicting part of the chunk cache if you have more than ~6000 cells . another issue to discuss is the row- or column- major layout. this would focus on the low-level implementation question, the more high level discussion of the convention should remain in 19. hdf5 is neither, but loading data from hdf5 files through h5py will result in row-major numpy arrays. all of these questions also apply to connecting/creating loom files through r.