 URL : "https://github.com/01org/mkl-dnn/issues/167" TITLE : batch norm example? BODY : hi, i'm having trouble implementing batch norm for forward inference. i've tried many things and they all lead to a mkldnn::error exception being thrown at some point or another, or incorrect results. i think i am not getting the various dimensions or formats of the memory descriptors correct. based on reading the documentation, i inferred that for a fc layer, we have to compress all the dimensions for a single input into the channel dimension. that seems to be what the batch norm descriptor expects too, since if i try any other combination, i get an error with the expected sizes of the memories being different. let me know if this is wrong. here's basically what i'm trying: https://gist.github.com/xyzsam/f03124689ffbd92c03eb3ede7fab0a25 the mkldnn::error gets thrown at the very end when i create the batch_normalization_forward primitive. but i don't get any other information about what caused it. in general, it would be really useful if these errors could get propagated up. if i take out the use_global_stats flag and remove the mean and variance memories, it at least runs to completion, but the output is wrong.