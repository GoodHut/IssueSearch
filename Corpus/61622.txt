 URL : "https://github.com/rdeits/ConditionalJuMP.jl/issues/32" TITLE : better relu formulation BODY : as pointed out by @vtjeng, doing y = @switch x <= 0 => 0, x >= 0 => x results in a model whose convex relaxation isn't as tight as it should be specifically, it misses the constraints that y >= 0 and y >= x . can we do better? i think the ideal result would be: y >= x y >= 0 y <= x + m 1 - z y <= mz