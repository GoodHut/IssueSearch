 URL : "https://github.com/scikit-learn/scikit-learn/issues/10230" TITLE : scoring in gridsearchcv BODY : it seems that gridsearchcv collects the _scores_ of its inner cross-validation folds and then averages across the scores of all folds. i was wondering about the rationale behind this. at first glance, it would seem more flexible to instead collect the _predictions_ of its cross-validation folds and then apply the chosen scoring metric to the predictions of all folds. the reason i stumbled upon this is that i use gridsearchcv on an imbalanced data set with cv=leaveoneout and scoring='balanced_accuracy' scikit-learn v0.20.dev0 . it doesn't make sense to apply a scoring metric such as balanced accuracy or recall to each left-out sample. rather, i would want to collect all predictions first and then apply my scoring metric once to all predictions. or does this involve an error in reasoning?