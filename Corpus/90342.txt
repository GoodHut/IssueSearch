 URL : "https://github.com/bkeepers/webhook-proxy/issues/4" TITLE : server deployment options BODY : ðŸ‘‹ @bkeepers @jasonetco i really like this idea and think it's pretty maintainable from an application security aspect, but i wanted to open this issue to talk about how we might deploy it, given that it requires a server with a long-running connection that remains open. to summarize our conversation last night in slack: - heroku has a 55 second timeout, after which the client needs to reconnect - lambda/api gateway has a 30 second limit on http connections, but even then the event emitter didn't respond to any clients within that 30 second window - would be nice if the service had a way to persist received payloads, so the client could easily replay the last n number of events all this points to the need for a real server, but obviously we don't want to lose the benefit of easy deployment. i don't know what others have experience with, but a few options come to mind with the full awareness that i may be prematurely optimizing here : - dokku installed on a vm: https://github.com/dokku/dokku - pros: very heroku-like experience, based on docker under the hood - cons: not sure how scalable this is to multiple machines - setup a small kube cluster maybe just minikube on a single vm - pros: still package as a container, but potentially easier to manage and scale in the long term - cons: more initial setup, but at least i think it would be fun : - others? maybe just a simple script in ci is all we need initially and use something like for persistence, we can probably just use an in-memory cache like node-cache-manager https://github.com/bryandonovan/node-cache-manager without a store initially. maybe we add redis if we feel it's necessary. i'm happy to work on any of these options and help maintain it. just wanted to get my initial thoughts down after working with it.