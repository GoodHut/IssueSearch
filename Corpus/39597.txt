 URL : "https://github.com/dw/py-lmdb/issues/155" TITLE : slow sequential read BODY : i have trouble getting reasonable sequential reads from lmdb, i think it might be due to the kind of data i store in it, that is not suited for lmdb? my best guess would be that the data is somehow fragmented? the database is ~30gib every key is a 16byte/128bit uuid stored as binary , the values are some msgpack encoded data that is ~150kib in size. here are the stats does the overflow_pages point to fragmentation? : {'overflow_pages': 7519896, 'branch_pages': 18, 'leaf_pages': 2392, 'psize': 4096, 'depth': 3, 'entries': 197892} i load the data sequentially using the following code no decoding or anything is performed : db = lmdb.open filename, map_size=1024 4 with db.begin write=false as txn: cursor = txn.cursor for key, value in cursor: pass this takes ~10minutes 11.65s user 110.60s system 20% cpu 10:06.38 total which would roughly equal 53.69 mib/s read speed. for comparison i read the file using cat db/file.db > /dev/null in ~30seconds 0.04s user 12.86s system 37% cpu 34.803 total . the file is stored on a ssd that hdparm shows capable of delivering 435.09 mib/s timing buffered disk reads: 1306 mb in 3.00 seconds = 435.09 mb/sec . similar much smaller files are much faster to read. is it the random uuid that i use as a key that is causing this? what can i do to fix this, use a sequential integer as a key instead? affected operating systems linux affected py-lmdb version 0.92 py-lmdb installation method pip install lmdb using bundled or distribution-provided lmdb library? bundled distribution name and lmdb library version 0, 9, 18 machine free -m output total used free shared buff/cache available mem: 32116 13976 256 630 17883 17048 swap: 16383 429 15954