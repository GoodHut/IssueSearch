 URL : "https://github.com/tensorflow/nmt/issues/225" TITLE : beam search optimization BODY : after experimentation on a nmt-realted tensorflow project - https://github.com/ssampang/im2latex - i realized that my greedy embedding decoder using the greedyembeddinghelper performed better than my beamsearchdecoder . according to this paper https://aclweb.org/anthology/d16-1137 , this is probably due to my training strategy that doesn't account for exposure bias : the model is never exposed to its own errors during training. indeed, i am using the seq2seq traininghelper that only trains one step ahead, and the gold standard is used every time to predict the next sequence output. instead the paper claims significant improvements from a beam search optimized training strategy. do you know if using the scheduledembeddingtraininghelper would suffice to account for this exposure bias ? because this looks really close to what is done in the paper, except that only one trajectory would be considered here.