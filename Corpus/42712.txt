 URL : "https://github.com/thedatahub/Datahub-Factory/issues/42" TITLE : bulk import into solr BODY : at this point, the solr module will push records synchronously to the index. these calls are blocking. which means each call takes a set amount of time to push, process and return to the factory before the next call is executed. this approach is usable for small sets of data ie. 2.000 records but not for 100.000 records. apache solr provides import handlers https://cwiki.apache.org/confluence/display/solr/uploading+data+with+index+handlers to import a bulk set of data into the index. instead of pushing discrete json objects as http messages, a json formatted file containing all the records is prepared and pushed to a separate api endpoint. this will trigger a direct, fast import and index process within solr. 6.000+ records are easily imported in < 2 secs this way. however. this is a 2 step process: 1. generate a json file from a data source using a fix. the schema adheres to the solr schema. 2. push the json file to the solr index via the import handler. in our current setup, a pipeline configuration will result in the generation of a json file. the architecture doesn't automate the second step. this still needs to be done manually. so, question is: should / could / how do we integrate this in the factory? several options: we create a totally separate, dedicate command easy to do, but very specific and not flexible we add support for pre / post processors in the transport command which act as handles throughout the pipeline via event listeners. we create a custom exporter in the arthub module which does the outputting to json and the push to solr in one move.