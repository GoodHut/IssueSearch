 URL : "https://github.com/SeanNaren/deepspeech.pytorch/issues/154" TITLE : relu and batchnorm BODY : the ds-2 paper https://arxiv.org/pdf/1512.02595.pdf describes using hardtanh as the activation function of rnns and applying batch norm on the output of the rnn modules. i don't think the code follows this implementation. does it follow? currently, the code uses hardtanh on the output of conv layers, not rnn default is lstm in the code layers.