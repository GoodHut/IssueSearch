 URL : "https://github.com/thtrieu/darkflow/issues/234" TITLE : batch norm in config BODY : hi , we are using a config which uses batch norm. we want to take the ckpt and port it over to low level gpu calls on . mobile for prediction. these are the trainable variables i observe 0-convolutional/biases:0 16, 0-convolutional/kernel:0 3, 3, 3, 16 0-convolutional/gamma:0 16, 0-convolutional/moving_mean:0 16, 0-convolutional/moving_variance:0 16, on freezing the graph, i see the following operations i cant see variables anymore when frozen 0-convolutional/filter 0-convolutional sub/y sub truediv/y truediv mul/y mul biasadd/bias biasadd i am wondering if savepb is folding batch norm variables already in to weights and biases ? i dont see it from the code. assuming it does not fold, it would make it imperative that we do it ourselves. i dont see a beta variable which is needed to do the folding. a = ùõÑ / ‚àö s + 0.001 , b = √ü - a m w = w a s: variance m: mean ùõÑ: gamma √ü: beta w: weights of a feature channel b: bias of a feature channel w: batch nomalized weights so i dont know how to compute this. any help is appreciated