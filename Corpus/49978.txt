 URL : "https://github.com/pymanopt/pymanopt/issues/38" TITLE : user defined gradient BODY : hello, i define my gradient i usually cannot use automatic differentiation for problems i am interested in , but i still get an error because of a missing backend. can you help with bypassing automatic differentiation: python def cost x : return x 0 -0.5 2+x 1 2 def egrad x : return 2 x 0 -0.5 ,2 x 1 m=euclidean 2 solver=conjugategradient problem = problem manifold=m, cost=cost,grad=egrad, egrad=egrad,verbosity=2 xoptimal = solver.solve problem,x= 10,10 compiling cost function... ... valueerror: cannot determine autodiff backend from cost function of type function . available backends are: thanks!