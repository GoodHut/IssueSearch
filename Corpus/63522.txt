 URL : "https://github.com/nengo/nengo/issues/1255" TITLE : effective learning rate BODY : when implementing a _learn every n-th timestep_ feature from 387 it was decided that the delta signal should be scaled to help keep learning at a similar rate to the normal learning implementation. this opened up discussion about how the learning rate should be scaled and what key parameters should be taken into account. if there is time and interest, this is a spot to discuss the _effective learning rate_ and the interactions between learning rate and parameters like radius, dt, n_neurons, apply_every, etc. ideally, we could add documentation and/or an example that shows how these parameters interact and help build some intuition about these things.