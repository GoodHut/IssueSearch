 URL : "https://github.com/miyosuda/async_deep_reinforce/issues/49" TITLE : incorrect policy loss BODY : hi, i'm reading this repository to implement my own a3c. then i found policy loss incorrect. current policy loss is at https://github.com/miyosuda/async_deep_reinforce/blob/master/game_ac_network.py l31 . policy entropy entropy = -tf.reduce_sum self.pi log_pi, reduction_indices=1 policy loss output adding minus, because the original paper's objective function is for gradient ascent, but we use gradient descent optimizer. policy_loss = - tf.reduce_sum tf.reduce_sum tf.multiply log_pi, self.a , reduction_indices=1 self.td + entropy entropy_beta so it's like policy_loss = -log pi a + beta entropy . in this case, entropy would be minimized.however, entropy should be maximized to avoid convergence for exploration. original paper https://arxiv.org/pdf/1602.01783.pdf says, ! image https://user-images.githubusercontent.com/5235131/32336726-65c9ae4a-c033-11e7-9e0f-c755fbec9551.png thus, correct policy loss should be policy_loss = -log pi a - beta entropy . if i am wrong, just please close this issue. i hope this helps you improve this implementation. @miyosuda