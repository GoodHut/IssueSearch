 URL : "https://github.com/requests/requests/issues/4191" TITLE : possible memory leak BODY : i'm crawling a lot of different urls using the requests library and i encountered that the process takes more and more ram over time. basically all i do is calling this iteratively from multiple threads: {python} r = requests.get url=url, timeout=timeout content = r.text when i comment out the second line this issue does not occur ... am i using the library fundamentally wrong or could this actually be a bug?