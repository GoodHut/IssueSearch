 URL : "https://github.com/samuelli/bot-render/issues/14" TITLE : thinking out loud... BODY : hi i have a react app i want to expose to search bots but i don't believe in using react ssr because of performance concerns as well as having to do route-based page generation across server and client screams of incidental complexity my initial thought was upon receiving a request the server reads the spa.html file into a string and inject into it the data for the initial requested route which is specified in the hash fragment as js vars for json and text data , hidden div for text data and dynamic meta tags. this way when the spa is loaded by the browser the initial route will already have the data, so it will start rendering without having to wait for the data, so the initial render will be fast, assuming the biggest payload is the data i know some spas use a million npm modules and thus have bigger more fundamental engineering problems than can be solved by ssr, but i'm talking about well engineered react apps. in addition, the search bots will have the text content in hidden div and the meta tags to go along with the content. after the initial requested route is served, the spa takes over. if user is authenticated, none of the above happens and the spa.html file is served statically. that seemed like a good solution until i came across the idea of using headless chrome to serve bot requests. can you tell me why the headless chrome approach would be more robust? or better in any way? also, i see a pr that's still open, so i'm wondering about the maturity of this approach. any insight would help greatly. thank you,