 URL : "https://github.com/BTAA-Geospatial-Data-Project/geoportal/issues/6" TITLE : webcrawlers / robots.txt BODY : robots.txt needs more work. some examples of stuff from the logs, not sure where the links originate: get /catalog.html?f%5bdc_subject_sm%5d%5b%5d=highways&per_page=100 http/1.1 200 36083 - mozilla/5.0 compatible; googlebot/2.1; +http://www.google.com/bot.html /catalog.rss?f%5bdc_subject_sm%5d%5b%5d=boundaries&f%5blayer_geom_type_s%5d%5b%5d=raster http/1.1 200 655 - mozilla/5.0 compatible; ahrefsbot/5.1; +http://ahrefs.com/robot/ get /catalog.html?f dc_publisher_sm =indiana+election+division&f dct_ispartof_sm =indianamap&f dct_spatial_sm =indiana&per_page=10&sort=solr_year_i+desc%252c+dc_title_sort+asc http/1.1 302 87 - mozilla/5.0 compatible; baiduspider/2.0; +http://www.baidu.com/search/spider.html get /catalog/facet/dc_publisher_sm?f%5bdct_ispartof_sm%5d%5b%5d=minnesota+geospatial+commons http/1.1 200 14524 - mozilla/5.0 compatible; googlebot/2.1; the above aren't so bad - just a few hundred per day, and at least one robot i don't recognize ignoring the disallow: /?f but the major crawlers are behaving