 URL : "https://github.com/dask/fastparquet/issues/72" TITLE : writing massive files BODY : i'm using fastparquet and it's great so far. my only problem is i need to write massive files to disk. due to how fastparquet.write method works i need to prepare the whole dataframe in memory which doesn't fit even in the really expensive azure vm. for me writing to separate files is not an option customer requirements . is there something you can recommend when it can be written in chunks i.e. one series at a time? much appreciated.