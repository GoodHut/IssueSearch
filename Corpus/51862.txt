 URL : "https://github.com/jimsalterjrs/sanoid/issues/93" TITLE : broken pipe workaround BODY : i've had a lot of issues with broken pipes when doing large sends . some of those i've managed to resolve turning off smart qos features , and some i haven't yet. from what i can determine, the only way to approach this in a way that will always work is to pipe zfs send to file, chunk it, rsync the chunks, and then zfs receive on the other end. of course, it would have to be buffered so it doesn't consume the entire filesystem's storage space in the process. as a side-note, i investigated mosh https://mosh.org/ with the hope that it would support pipes, but unfortunately it doesn't. i found that someone had already written something in php which automates exactly this see here http://list.zfsonlinux.org/pipermail/zfs-discuss/2016-may/025653.html and here http://list.zfsonlinux.org/pipermail/zfs-discuss/2014-march/014859.html . it maintains a configurable buffer of a certain number of chunks, uses rsync to transmit those, and then receives more data from the zfs send pipe to create more chunks to repeat the process until finished. apparently it can survive long periods of being disconnected, ip address changes, etc. the send and receive pipes are established as separate processes on both sides, so that a single broken ssh connection can't break the entire pipe. i was thinking about bolting that onto syncoid, but from another comment, the author of the above indicated that it apparently doesn't support pull replication yet. i've reached out to them to see if that's still the most current copy or not. unless the author has a more current version, i was thinking about reimplementing something similar in perl as a modification to syncoid for my own use which i'd be happy to see integrated . i thought i'd open something here to see what anyone else's thoughts are on this approach before i get too far. am i missing an easier/better solution? would this be good to have as an optional mode of operation in syncoid? thanks!