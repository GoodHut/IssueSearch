 URL : "https://github.com/clab/dynet/issues/542" TITLE : dropout speed issues BODY : hi there! i have been experiencing some issues using dropout. the thing is that it makes my system many times slower to train. before dropout it took ~6h in gpu to run a full epoch. now, it takes ~24h. that's a 4 fold increase. the only change in my code was do use dropout in the hidden layers of the net. also, gpu usage is now way down, and a lot of time it actually reaches 0, which didn't happen before. any clues?