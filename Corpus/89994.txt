 URL : "https://github.com/Merck/Halyard/issues/27" TITLE : resume bulkload on abort? BODY : just had a situation, when the mapreduce job finished processing rdf and saving it to /tmp folder. however, my hbase server stopped working and ./bulkload reported error and quit. this scenario is unlikely to happen in production cluster redundant zookeeper + redundant hbase , however for local clusters it would nice to have ./bulkload split into two phases: 1. mapreduce data to hbase tables i.e. saving it to /tmp folder 2. load /tmp folder into hbase table bla right now, if i want to continue ./bulkload it simply throws stating that /tmp folder already exists.