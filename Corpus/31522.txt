 URL : "https://github.com/hmmlearn/hmmlearn/issues/209" TITLE : some buggy problems BODY : 1. when i train a gmmhmm, why does the last training sample always have the largest log probability? training set scores: user1: -18.748281177 -20.0785920394 -26.804748964 -16.5073564693 -1.78057805812 user2: -7.7311778362 -9.95478036866 -7.92847595956 -7.81001851123 0.0306164459485 user3: -11.0709910571 -11.8985166449 -11.517120555 -11.4172607698 -2.52482553686 user4: -15.091849959 -7.80029158842 -14.7746975112 -7.74823067649 1.22052400061 ... 2. when i try to decode the states of a left-to-right gmmhmm, why would such cases happen: 0 0 0 5 5 5 5 5 5 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 2 2 2 2 2 2 2 2 0 2 2 2 2 4 4 4 4 0 5 5 1 1 1 1 6 4 4 4 4 4 4 4 0 5 5 5 5 5 5 5 5 5 6 6 6 6 6 3 0 6 6 6 6 6 6 3 1 1 6 0 0 0 0 0 0 0 0 1 1 1 1 0 0 5 5 5 5 5 5 5 5 1 6 1 1 1 1 1 1 1 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 4 3 3 3 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 3 1 4 4 4 0 0 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 5 1 the last state happens at the first time step... this is the start probability: 'startprob_:', array 1., 0., 0., 0., 0., 0., 0.