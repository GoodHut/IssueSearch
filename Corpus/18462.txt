 URL : "https://github.com/yjxiong/temporal-segment-networks/issues/67" TITLE : test image number BODY : i see 25 rgb frames or optical flow stacks were sampled for testing, but in the train_val.prototxt, you still used segment temporal in video, so i'm wondering how it is tested？ in the paper： we follow the testing scheme of the original two-stream convnets 1 , where we sample 25 rgb frames or optical ow stacks from the action videos. meanwhile, we crop 4 corners and 1 center, and their horizontal ipping from the sampled frames to evaluate the convnets. for the fusion of spatial and temporal stream networks, in the train_val.prototxt: layer { name: data type: videodata top: data top: label video_data_param { source: data/ucf101_rgb_val_split_3.txt batch_size: 1 new_length: 1 num_segments: 3 modality: rgb name_pattern: img_%05d.jpg } transform_param{ crop_size: 224 mirror: false mean_value: 104, 117, 123, 104, 117, 123, 104, 117, 123 } include: { phase: test } }